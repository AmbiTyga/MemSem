{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MemSem-Github.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1xwcC7Lv_HNU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5n0DitXluNg",
        "colab_type": "text"
      },
      "source": [
        "## Necessay pip packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXcuaOZ90q3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install praw\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install transformers\n",
        "!pip install tensorflow==2.1.0-rc0\n",
        "!pip install pandas \n",
        "!pip install numpy\n",
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scouJNvbktJ9",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2uE0K8KjEuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import praw\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import getpass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Input, Dropout, Flatten\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import image as keras_image\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from collections import defaultdict\n",
        "import regex as re\n",
        "from keras.preprocessing import image as keras_image\n",
        "import pickle\n",
        "import pytesseract\n",
        "import shutil\n",
        "import random\n",
        "try:\n",
        " from PIL import Image\n",
        "except ImportError:\n",
        " import Image\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xwcC7Lv_HNU",
        "colab_type": "text"
      },
      "source": [
        "## Image downloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFz7FHvJ0zCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class redditImageScraper:\n",
        "    def __init__(self):\n",
        "        self.client_id = str(getpass.getpass(\n",
        "            \"For Client ID and Client secret go to\\n https://www.reddit.com/prefs/apps \\n and create your token\\nEnter Your Client ID:\\n\"))\n",
        "\n",
        "        self.client_secret = str(getpass.getpass(\"\\nEnter your client Secret:\\n\"))\n",
        "        # print(client_id,client_secret)\n",
        "        self.reddit = praw.Reddit(client_id=self.client_id,\n",
        "                                  client_secret=self.client_secret,\n",
        "                                  user_agent='my user agent')\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def download(self, image):\n",
        "        r = requests.get(image['url'])\n",
        "        with open(image['fname'], 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        print(\"File Downloaded: {} File Path: {} \".format(image['url'], image['fname']))\n",
        "        self.images['filepath'].append(image['fname'])\n",
        "\n",
        "    def start(self,path,page):\n",
        "        self.subreddit = self.reddit.subreddit(page)\n",
        "        if page == 'Dark_memes':\n",
        "          self.subreddit.quaran.opt_in()\n",
        "        self.imageGen = self.subreddit.top(limit=50)\n",
        "        self.images = {'filepath': []}\n",
        "        image = []\n",
        "        print('Started')\n",
        "        try:\n",
        "\n",
        "            for submission in self.imageGen:\n",
        "\n",
        "                if submission.url.endswith(('jpg', 'jpeg', 'png')):\n",
        "\n",
        "                    fname = path + re.search('(?s:.*)\\w/(.*)', submission.url).group(1)\n",
        "                    if not os.path.isfile(fname):\n",
        "                        image.append({'url': submission.url, 'fname': fname})\n",
        "\n",
        "            if len(image):\n",
        "                if not os.path.exists(path):\n",
        "                    os.makedirs(path)\n",
        "                with concurrent.futures.ThreadPoolExecutor() as ptolemy:\n",
        "                    ptolemy.map(self.download, image)\n",
        "        except Exception as e:\n",
        "            if e=='received 403 HTTP response' :\n",
        "              print(\"Page is quarantined or you have'nt joined it yet\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhBYm3gE03f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloader = redditImageScraper()\n",
        "downloader.start(path='./dataset/negative/',page  = 'Dark_memes')\n",
        "downloader.start(path='./dataset/positive/', page= 'wholesomememes')\n",
        "\n",
        "downloader.start(path='./dataset/neutral/', page = 'antimeme')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUEtky_H3OdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlrgPMn2_PxG",
        "colab_type": "text"
      },
      "source": [
        "# Text Extractor and preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrd9WMekHBU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(filepath):\n",
        "  image = keras_image.load_img(\n",
        "                  filepath,\n",
        "                  target_size=(224,224),\n",
        "                  interpolation ='bicubic'))\n",
        "  img = image.copy()\n",
        "  img = keras_image.img_to_array(img)\n",
        "  img = img//255.0\n",
        "  return img\n",
        "\n",
        "def preprocess_txt(text):\n",
        "  tag_map = defaultdict(lambda: wn.NOUN)\n",
        "  tag_map['J'] = wn.ADJ\n",
        "  tag_map['V'] = wn.VERB\n",
        "  tag_map['R'] = wn.ADV\n",
        "  word_Lemmatized = WordNetLemmatizer()\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"\\n\",\" \",text)\n",
        "  text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  stop = stopwords.words('english')\n",
        "  pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
        "  text = text.replace(pat, '')\n",
        "  text = text.replace(r'\\s+', ' ')\n",
        "  text = re.sub(r'[^a-zA-Z0-9 -]', '', text) \n",
        "  text = re.sub('@[^\\s]+','',text)\n",
        "  text = word_tokenize(text)\n",
        "  Final_words = []\n",
        "  for word, tag in pos_tag(text):\n",
        "      word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
        "      Final_words.append(word_Final)\n",
        "  text = \" \".join(Final_words)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxMgQVR_POM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {\"image\":[],\"filepath\":[],\"text\":[],\"label\":[]}\n",
        "for dirname,_,filenames in os.walk('./dataset/'):\n",
        "  for filename in filenames:\n",
        "    try:\n",
        "      if dirname == './dataset/positive':\n",
        "        data['label'].append(int(1))\n",
        "      if dirname == './dataset/neutral':\n",
        "        data['label'].append(int(2))\n",
        "      if dirname == './dataset/negative':\n",
        "        data['label'].append(int(0))\n",
        "      \n",
        "      data['image'].append(\n",
        "          preprocess_image(\n",
        "              os.path.join(dirname,filename)))\n",
        "      \n",
        "      data['filepath'].append(\n",
        "          os.path.join(dirname,filename))\n",
        "      \n",
        "      data['text'].append(\n",
        "          preprocess_txt(\n",
        "              pytesseract.image_to_string(\n",
        "                  Image.open(\n",
        "                      os.path.join(dirname,filename)))))\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue\n",
        "    \n",
        "\n",
        "    print('\\r images: {} texts: {} labels : {}'.format(len(data['image']),len(data['text']),len(data['label'])),end='')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zJixt_br6C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with open(\"data.pkl\",\"wb\") as pickle_out:\n",
        "  pickle.dump(data, pickle_out)\n",
        "pickle_out.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkMO4zzqtypB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaTak4d7F42_",
        "colab_type": "text"
      },
      "source": [
        "# Multimodal Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95GFch-2HZMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier:\n",
        "\n",
        "    def __init__(self, epochs=32, batch_size=64,metrics = False, plot_model_diagram=False, summary=False):\n",
        "        self.epochs = epochs\n",
        "        self.metrics = metrics\n",
        "        self.batch_size = batch_size\n",
        "        self.plot_model_diagram = plot_model_diagram\n",
        "        self.summary = summary\n",
        "        self.seq_len = 42\n",
        "        self.bert_layer = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.vgg = VGG19(weights='imagenet', include_top=False)\n",
        "        self.bert_layer.trainable = False\n",
        "        self.vgg.trainable = False\n",
        "\n",
        "    def encode(self, texts):\n",
        "        input_id = []\n",
        "        token_type_id = []\n",
        "        attention_mask = []\n",
        "        for text in texts:\n",
        "            dictIn = self.tokenizer.encode_plus(text, max_length=self.seq_len, pad_to_max_length=True)\n",
        "            input_id.append(dictIn['input_ids'])\n",
        "            token_type_id.append(dictIn['token_type_ids'])\n",
        "            attention_mask.append(dictIn['attention_mask'])\n",
        "        return np.array(input_id), np.array(token_type_id), np.array(attention_mask)\n",
        "\n",
        "    def labelencoder(self, labels):\n",
        "        new_label = np.zeros((len(labels), 3))\n",
        "        for i, label in enumerate(labels):\n",
        "            if label == 0:\n",
        "                new_label[i] = [0, 0, 1]\n",
        "            elif label == 1:\n",
        "                new_label[i] = [0, 1, 0]\n",
        "            elif label == 2:\n",
        "                new_label[i] = [1, 0, 0]\n",
        "\n",
        "        return new_label\n",
        "\n",
        "    def build(self):\n",
        "        input_id = Input(shape=(self.seq_len,), dtype=tf.int64)\n",
        "        mask_id = Input(shape=(self.seq_len,), dtype=tf.int64)\n",
        "        seg_id = Input(shape=(self.seq_len,), dtype=tf.int64)\n",
        "\n",
        "        _, bert_out = self.bert_layer([input_id, mask_id, seg_id])\n",
        "        dense = Dense(768, activation='relu')(bert_out)\n",
        "        dense = Dense(256, activation='relu')(dense)\n",
        "        txt_repr = Dropout(0.4)(dense)\n",
        "        ################################################\n",
        "        img_in = Input(shape=(224, 224, 3))\n",
        "        img_out = self.vgg(img_in)\n",
        "        flat = Flatten()(img_out)\n",
        "        dense = Dense(2742, activation='relu')(flat)\n",
        "        dense = Dense(256, activation='relu')(dense)\n",
        "        img_repr = Dropout(0.4)(dense)\n",
        "        concat = Concatenate(axis=1)([img_repr, txt_repr])\n",
        "        dense = Dense(64, activation='relu')(concat)\n",
        "        out = Dense(3, activation='softmax')(dense)\n",
        "        model = Model(inputs=[input_id, mask_id, seg_id, img_in], outputs=out)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=Adam(2e-5),\n",
        "                      metrics=['accuracy', precision, recall, f1]) if self.metrics else model.compile(\n",
        "            loss='categorical_crossentropy', optimizer=Adam(2e-5), metrics=['accuracy'])\n",
        "\n",
        "        plot_model(model) if self.plot_model_diagram else None\n",
        "        model.summary() if self.summary else None\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, data, validation_split=0.2):\n",
        "\n",
        "        model = self.build()\n",
        "        input_id, token_type_id, attention_mask = self.encode(data['text'])\n",
        "        image_data = np.asarray(data['image'])\n",
        "        labels = self.labelencoder(data['label'])\n",
        "\n",
        "        self.history = model.fit([input_id, token_type_id, attention_mask, image_data],\n",
        "                                 labels,\n",
        "                                 validation_split=validation_split,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 epochs=self.epochs)\n",
        "\n",
        "        model.save_weights('./model/MemSem')\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        model = self.build()\n",
        "        model.load_weights('./model/MemSem')\n",
        "        input_id, token_type_id, attention_mask = self.encode(data['text'].apply(preprocess_txt))\n",
        "        image_data = data['image'].apply(preprocess_image)\n",
        "        eval_data = [input_id, token_type_id, attention_mask,image_data]\n",
        "        labels = self.labelencoder(data['label'])\n",
        "        evaluation = model.evaluate(eval_data, labels)\n",
        "        return evaluation\n",
        "\n",
        "\n",
        "    def predict(self, image_path='./dataset/test/text.jpg', text=\"\"):\n",
        "\n",
        "        try:\n",
        "            model = self.build()\n",
        "            model.load_weights('./model/MemSem')\n",
        "            input_id, token_type_id, attention_mask = self.encode([preprocess_txt(text)])\n",
        "            image_data = preprocess_image(\n",
        "                keras_image.load_img(image_path,\n",
        "                                     target_size=(224, 224),\n",
        "                                     interpolation='bicubic'))\n",
        "            image_data = np.expand_dims(image_data, axis=0)\n",
        "            value = model.predict([input_id, token_type_id, attention_mask, image_data])\n",
        "\n",
        "            prediction = np.argmax(value)\n",
        "            if prediction == 2:  # negative = [0,0,1]\n",
        "                print(\"Its a bad meme\")\n",
        "            elif prediction == 1:  # postive = [0,1,0]\n",
        "                print(\"Its not bad XD\")\n",
        "            elif prediction == 0:  # neutral = [1,0,0]\n",
        "                print(\"Its meaningless\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2bhA7f4liIH",
        "colab_type": "text"
      },
      "source": [
        "## Model object and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4_2sanmJzd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls = Classifier(epochs =6,batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKX1JTz-hv1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"data.pkl\",\"rb\") as pickle_in:\n",
        "  final = pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "\n",
        "cls.train(final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsyT-odGLqMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(cls.history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUMVeH6bWxDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}